---
title: "lab07"
author: "Dylan Scoble"
date: "3/4/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(knitr)
```


```{r}
spotify <- read_csv("spotify.csv")
glimpse(spotify)
```

# Part 1: Data Prep & Modeling


### Exercise 1
```{r}
spotify <- spotify %>% 
  drop_na() %>% 
  mutate(target = as.factor(target),
         key = case_when(
           key == 2 ~ "D",
           key == 3 ~ "D#",
           TRUE ~ "Other"
         ))

glimpse(spotify)
```

```{r}
ggplot(data = spotify, aes(x = key, fill = target)) +
  geom_bar(position = "fill") +
  labs(title="Target vs Key",
       y="Proportion")
```
The plot above describes the relationship between Key and Target. For all songs in the key of D, about 60% of them have a target value of 1. For all songs in the key of D#, about 30% of them have a target value of 1. For all other songs, about half of them have a target value of 1.

### Exercise 2

```{r}
model <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence, data = spotify, family = binomial)

tidy(model, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(format="markdown", digits = 5)
```

### Exercise 3

```{r}
model_with_key <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence + key, data = spotify, family = binomial)

summary(model)$aic

summary(model_with_key)$aic

```

Adding key to the model lowers the model's AIC, making it better at predicting target. Therefore, we will continue to use the model with key.

### Exercise 4

```{r}
tidy(model_with_key, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(format="markdown", digits = 5)

```

If the key of the observation is D#, the target value's prediction will increase, by a value of exp(-1.07319).

# Part 2: Checking Assumptions

### Exercise 5

```{r}
df <- augment(model_with_key, type.predict = "response",type.residuals = "deviance")
glimpse(df)
```

### Exercise 6

```{r}
arm::binnedplot(x = df$.fitted, y = df$.resid,
                xlab = "Predicted Probabilities", 
                ylab = "Residuals",
                main = "Binned Residual vs. Predicted Values", 
                col.int = FALSE)
```

### Exercise 7

```{r}
arm::binnedplot(x = df$danceability, y = df$.resid,
                xlab = "Danceability", 
                ylab = "Residuals",
                main = "Binned Residual vs. Danceability", 
                col.int = FALSE)
```

### Exercise 8

```{r}
df %>% 
  mutate(key_resid = if_else(.resid > 1 | .resid < -1, "High Residual", "Low Residual")) %>% 
  group_by(key, key_resid) %>% 
  summarise(n = n()) %>%
  kable(format="markdown")
```

### Exercise 9

The linearity assumption is not satisfied because there is no clear linear relationship between our fitted values and our predictors, as shown by the plot in exercise 6.

# Part 3: Model Assessment & Prediction

### Exercise 10

```{r}
roc_curve <- ggplot(df, aes(d = as.numeric(target), m = .fitted)) +
  geom_roc(n.cuts = 10, labelround = 3) + 
  geom_abline(intercept = 0) + 
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)")

roc_curve
```
```{r}
calc_auc(roc_curve)$AUC
```

### Exercise 11

This model does effectively differentiate between the songs the user likes versus those he or she doesnâ€™t. The ROC Curve is entirely above the line x=y, which represents an entirely random classifier. Additionally, the area under the ROC curve exceeds 0.5.

### Exercise 12.

I would choose a threshold of 0.58 because it maximizes the TPR (true positive rate) while keeping FPR (false positive rate) as small as possible. In terms of the ROC curve, the 0.58 threshold seems to have the largest euclidean distance from the line x=y.

### Exercise 13

```{r}
threshold = 0.58
df %>% 
  mutate(prediction = if_else(.fitted < threshold, "0:No","1:Yes")) %>% 
  group_by(target, prediction) %>% 
  summarise(n = n()) %>%
  kable(format="markdown")
```

### Exercise 14

The proportion of true positives is $(512 / 512+508) = 0.502$

The proportion of false positives is $(508 / 512+508) = 0.498$

The missclassification rate is $(508 + 161 / 508 + 161 + 512 + 836) = 0.302$


